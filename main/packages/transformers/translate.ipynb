{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "Deep learning models can `translate` text from one language to another.  \n",
    "One popular library for machine translation is transformers by `Hugging` Face.  \n",
    "It provides easy-to-use interfaces to various `pre-trained` models, including those for translation tasks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you?\n",
      "Bună, ce mai faci?\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    pip install transformers\n",
    "    pip install sentencepiece\n",
    "    pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/torch_stable.html\n",
    "    pip install sacremoses\n",
    "\"\"\"\n",
    "\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer \n",
    "model_name = 'Helsinki-NLP/opus-mt-en-ro'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Text to translate\n",
    "text = \"Hello, how are you?\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Perform translation\n",
    "translated = model.generate(**inputs)\n",
    "\n",
    "# Decode translated text\n",
    "translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "# Output result\n",
    "print(text)\n",
    "print(translated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "For loading models and tokenizers an `internet` connection is typically required.  \n",
    "The library `fetches` the necessary model files from the Hugging Face model hub.  \n",
    "\n",
    "Once you've loaded the model and tokenizer at least once and `cached` them locally,  \n",
    "subsequent uses of the same model or tokenizer can often work `offline`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aceasta este o propoziție de test pentru traducere.\n",
      "This is a test sentence for translation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load model and tokenizer \n",
    "model_name = 'BlackKakapo/opus-mt-ro-en'                    # Look Here\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Example sentence in Romanian\n",
    "sentence = \"Aceasta este o propoziție de test pentru traducere.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Perform translation\n",
    "translated = model.generate(**inputs)\n",
    "\n",
    "# Decode translated text\n",
    "translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "# Output result\n",
    "print(sentence)\n",
    "print(translated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing\n",
    "\n",
    "\n",
    "To perform batch processing for translation using transformers, you can gather `multiple` sentences.  \n",
    "You ca process them `together` using the generate method of your translation model.   \n",
    "This method should provide `efficiency` improvements compared to translating sentences one by one.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Aceasta este o propoziție de test pentru traducere. This is a test sentence for translation.\n",
      "1 Acesta este un alt exemplu de propoziție. This is another example of a sentence.\n",
      "2 Cât de rapid poate fi acest model? How fast can this model be?\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load model and tokenizer \n",
    "model_name = 'BlackKakapo/opus-mt-ro-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Example sentences in Romanian\n",
    "sentences = [\n",
    "    \"Aceasta este o propoziție de test pentru traducere.\",\n",
    "    \"Acesta este un alt exemplu de propoziție.\",\n",
    "    \"Cât de rapid poate fi acest model?\"\n",
    "]\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Perform translation\n",
    "translated = model.generate(**inputs)\n",
    "\n",
    "# Decode translated texts\n",
    "translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "# Output results\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(i, sentence, translated_texts[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://huggingface.co/docs/transformers/model_doc/marian  \n",
    "https://huggingface.co/BlackKakapo/opus-mt-ro-en  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
